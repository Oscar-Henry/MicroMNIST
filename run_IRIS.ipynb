{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.engine import Value\n",
    "from micrograd.nn import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimension:\n",
      "Training set =  100 100\n",
      "Test set =  50 50\n",
      "Dimension of input =  4\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "X = np.array(iris.data)\n",
    "y = np.array(iris.target)\n",
    "\n",
    "r_index= list(np.random.permutation(len(y)))\n",
    "\n",
    "X_train, y_train = X[r_index[:100]], y[r_index[:100]]\n",
    "X_test, y_test = X[r_index[100:]], y[r_index[100:]]\n",
    "\n",
    "print(\"Dataset dimension:\")\n",
    "\n",
    "print(\"Training set = \", len(X_train), len(y_train))\n",
    "print(\"Test set = \", len(X_test), len(y_test))\n",
    "\n",
    "print(\"Dimension of input = \", len(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n",
      "First 5 samples:\n",
      "Sample 1: [5.1 3.5 1.4 0.2] (Class: 0, Species: setosa)\n",
      "Sample 2: [4.9 3.  1.4 0.2] (Class: 0, Species: setosa)\n",
      "Sample 3: [4.7 3.2 1.3 0.2] (Class: 0, Species: setosa)\n",
      "Sample 4: [4.6 3.1 1.5 0.2] (Class: 0, Species: setosa)\n",
      "Sample 5: [5.  3.6 1.4 0.2] (Class: 0, Species: setosa)\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature names:\", iris.feature_names)\n",
    "print(\"Target names:\", iris.target_names)\n",
    "\n",
    "print(\"First 5 samples:\")\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i+1}: {X[i]} (Class: {y[i]}, Species: {iris.target_names[y[i]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes 15\n",
      "MLP of [Layer of [Softmax Neuron (4), Softmax Neuron (4), Softmax Neuron (4)]]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(4, [3])\n",
    "print('number of nodes', len(model.parameters()))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 3.0833045531652625, accuracy 35.0%\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "def loss():\n",
    "    inputs = [list(map(Value, xrow)) for xrow in X_train]\n",
    "    \n",
    "    # forward the model to get scores\n",
    "    scores = list(map(model, inputs)) \n",
    "    \n",
    "    # Cross-entropy Loss\n",
    "    losses = [(- score[yi].log()) for yi, score in zip(y_train, scores)]\n",
    "    total_loss = sum(losses) / len(losses)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = []\n",
    "    for yi, score in zip(y_train, scores):\n",
    "        argmax = -1\n",
    "        valmax = 0\n",
    "        for idx, scorei in enumerate(score):\n",
    "            if scorei.data > valmax:\n",
    "                valmax = scorei.data\n",
    "                argmax = idx\n",
    "        accuracy.append((argmax == yi))\n",
    "\n",
    "    acc = sum(accuracy) / len(accuracy)\n",
    "        \n",
    "    return total_loss, acc\n",
    "\n",
    "total_loss, acc = loss()\n",
    "print(f\"step {0} loss {total_loss.data}, accuracy {acc*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 loss 12.289526460019374, accuracy 36.0%\n",
      "step 2 loss 23.92474759790549, accuracy 32.0%\n",
      "step 3 loss 18.7361544165299, accuracy 32.0%\n",
      "step 4 loss 11.705742805898574, accuracy 32.0%\n",
      "step 5 loss 14.929623785721216, accuracy 36.0%\n",
      "step 6 loss 14.249479248016986, accuracy 32.0%\n",
      "step 7 loss 19.68421611758206, accuracy 32.0%\n",
      "step 8 loss 3.003324116950807, accuracy 38.0%\n",
      "step 9 loss 6.357328915001207, accuracy 32.0%\n",
      "step 10 loss 13.813564492022067, accuracy 34.0%\n",
      "step 11 loss 7.4900449795087605, accuracy 68.0%\n",
      "step 12 loss 0.17714346068017225, accuracy 93.0%\n",
      "step 13 loss 2.3312177058424575, accuracy 68.0%\n",
      "step 14 loss 10.5621864747883, accuracy 68.0%\n",
      "step 15 loss 0.1474070023467112, accuracy 95.0%\n",
      "step 16 loss 1.1458250761942816, accuracy 68.0%\n",
      "step 17 loss 10.321122883351766, accuracy 38.0%\n",
      "step 18 loss 6.964933999055595, accuracy 68.0%\n",
      "step 19 loss 3.032574672196091, accuracy 65.0%\n",
      "step 20 loss 8.103835072730948, accuracy 68.0%\n",
      "step 21 loss 1.435436262495156, accuracy 68.0%\n",
      "step 22 loss 9.366627230476134, accuracy 68.0%\n",
      "step 23 loss 0.17532994054837442, accuracy 95.0%\n",
      "step 24 loss 0.766951842831584, accuracy 68.0%\n",
      "step 25 loss 7.375908202451788, accuracy 68.0%\n",
      "step 26 loss 4.286099750548878, accuracy 68.0%\n",
      "step 27 loss 4.809774549653052, accuracy 68.0%\n",
      "step 28 loss 6.0107720545014995, accuracy 68.0%\n",
      "step 29 loss 2.3591028008665593, accuracy 68.0%\n",
      "step 30 loss 7.641380753312988, accuracy 68.0%\n",
      "step 31 loss 0.33810634191932615, accuracy 82.0%\n",
      "step 32 loss 3.86615451257606, accuracy 68.0%\n",
      "step 33 loss 4.150568545128834, accuracy 68.0%\n",
      "step 34 loss 5.473066652629528, accuracy 68.0%\n",
      "step 35 loss 1.860511618979101, accuracy 68.0%\n",
      "step 36 loss 6.802980864759933, accuracy 68.0%\n",
      "step 37 loss 0.2943557680735992, accuracy 84.0%\n",
      "step 38 loss 2.6751851940670583, accuracy 68.0%\n",
      "step 39 loss 4.390943211085255, accuracy 68.0%\n",
      "step 40 loss 4.253021620733764, accuracy 68.0%\n",
      "step 41 loss 2.1923819332735968, accuracy 68.0%\n",
      "step 42 loss 5.570240722775509, accuracy 68.0%\n",
      "step 43 loss 0.5120843953239271, accuracy 79.0%\n",
      "step 44 loss 3.3679546124947866, accuracy 68.0%\n",
      "step 45 loss 2.4975882194876364, accuracy 68.0%\n",
      "step 46 loss 4.691937348562897, accuracy 68.0%\n",
      "step 47 loss 0.7440100633540339, accuracy 75.0%\n",
      "step 48 loss 3.6053648432732666, accuracy 68.0%\n",
      "step 49 loss 1.5302891548602044, accuracy 69.0%\n",
      "step 50 loss 4.259480476419129, accuracy 68.0%\n",
      "step 51 loss 0.6168478830218415, accuracy 77.0%\n",
      "step 52 loss 2.655763529469193, accuracy 68.0%\n",
      "step 53 loss 1.8817706610325278, accuracy 69.0%\n",
      "step 54 loss 3.61541021036525, accuracy 68.0%\n",
      "step 55 loss 0.6668845198210954, accuracy 77.0%\n",
      "step 56 loss 2.2627417467834934, accuracy 68.0%\n",
      "step 57 loss 1.6121197974125487, accuracy 70.0%\n",
      "step 58 loss 2.9728438067861442, accuracy 68.0%\n",
      "step 59 loss 0.712527068745539, accuracy 77.0%\n",
      "step 60 loss 1.85958323004283, accuracy 69.0%\n",
      "step 61 loss 1.332162625791625, accuracy 71.0%\n",
      "step 62 loss 2.277571762367848, accuracy 68.0%\n",
      "step 63 loss 0.7729632638478803, accuracy 77.0%\n",
      "step 64 loss 1.4942672095713716, accuracy 69.0%\n",
      "step 65 loss 0.9902403656024644, accuracy 75.0%\n",
      "step 66 loss 1.516566051850989, accuracy 69.0%\n",
      "step 67 loss 0.7579462004224771, accuracy 77.0%\n",
      "step 68 loss 1.0602802198271, accuracy 74.0%\n",
      "step 69 loss 0.5749510415599121, accuracy 81.0%\n",
      "step 70 loss 0.6774720504726734, accuracy 80.0%\n",
      "step 71 loss 0.2580957881342314, accuracy 89.0%\n",
      "step 72 loss 0.22688847229224673, accuracy 91.0%\n",
      "step 73 loss 0.0805118465118358, accuracy 99.0%\n",
      "step 74 loss 0.07699905409896939, accuracy 98.0%\n",
      "step 75 loss 0.07536603333688366, accuracy 98.0%\n",
      "step 76 loss 0.07511794565540893, accuracy 98.0%\n",
      "step 77 loss 0.07502934579225397, accuracy 98.0%\n",
      "step 78 loss 0.074976188096842, accuracy 98.0%\n",
      "step 79 loss 0.0749303073634926, accuracy 98.0%\n",
      "step 80 loss 0.07488789290089655, accuracy 98.0%\n",
      "step 81 loss 0.07484827194058491, accuracy 98.0%\n",
      "step 82 loss 0.07481117152643373, accuracy 98.0%\n",
      "step 83 loss 0.07477638183272245, accuracy 98.0%\n",
      "step 84 loss 0.07474372268141848, accuracy 98.0%\n",
      "step 85 loss 0.07471303748436234, accuracy 98.0%\n",
      "step 86 loss 0.07468418967050297, accuracy 98.0%\n",
      "step 87 loss 0.07465705978176736, accuracy 98.0%\n",
      "step 88 loss 0.07463154305322493, accuracy 98.0%\n",
      "step 89 loss 0.07460754738531675, accuracy 98.0%\n",
      "step 90 loss 0.0745849916372084, accuracy 98.0%\n",
      "step 91 loss 0.07456380418462094, accuracy 98.0%\n",
      "step 92 loss 0.07454392169665054, accuracy 98.0%\n",
      "step 93 loss 0.07452528809481886, accuracy 98.0%\n",
      "step 94 loss 0.07450785366449555, accuracy 98.0%\n",
      "step 95 loss 0.07449157429431187, accuracy 98.0%\n",
      "step 96 loss 0.07447641082355942, accuracy 98.0%\n",
      "step 97 loss 0.07446232848108932, accuracy 98.0%\n",
      "step 98 loss 0.07444929640207573, accuracy 98.0%\n",
      "step 99 loss 0.07443728721131632, accuracy 98.0%\n",
      "step 100 loss 0.07442627666364152, accuracy 98.0%\n"
     ]
    }
   ],
   "source": [
    "# backward\n",
    "for k in range(100):\n",
    "    model.zero_grad()\n",
    "    total_loss.backward()\n",
    "        \n",
    "    # update (sgd)\n",
    "    learning_rate = 1.0 - 0.9   *k/100\n",
    "    for p in model.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    total_loss, acc = loss()\n",
    "        \n",
    "    print(f\"step {k + 1} loss {total_loss.data}, accuracy {acc*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(batch_size=None):\n",
    "    inputs = [list(map(Value, xrow)) for xrow in X_test]\n",
    "    \n",
    "    # forward the model to get scores\n",
    "    scores = list(map(model, inputs))\n",
    "    \n",
    "    # Cross-entropy Loss\n",
    "    losses = [(- score[yi].log()) for yi, score in zip(y_test, scores)]\n",
    "    total_loss = sum(losses) / len(losses)\n",
    "\n",
    "    # Accuracy\n",
    "    predict = []\n",
    "    accuracy = []\n",
    "    for yi, score in zip(y_test, scores):\n",
    "        argmax = -1\n",
    "        valmax = 0\n",
    "        for idx, scorei in enumerate(score):\n",
    "            if scorei.data > valmax:\n",
    "                valmax = scorei.data\n",
    "                argmax = idx\n",
    "        predict.append((argmax, yi))\n",
    "        accuracy.append((argmax == yi))\n",
    "\n",
    "    acc = sum(accuracy) / len(accuracy)\n",
    "        \n",
    "    return total_loss, acc, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.08012238980901966, accuracy = 98.0%\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "total_loss, acc, pred = test()\n",
    "print(f\"Loss = {total_loss.data}, accuracy = {acc*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
