{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.engine import Value\n",
    "from micrograd.nn import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimension:\n",
      "Training set =  120 120\n",
      "Test set =  30 30\n",
      "Dimension of input =  4\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "X = np.array(iris.data)\n",
    "y = np.array(iris.target)\n",
    "\n",
    "ratio_sets = 0.8\n",
    "size = int(150 * ratio_sets)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "r_index= list(np.random.permutation(len(y)))\n",
    "\n",
    "X_train, y_train = X[r_index[:size]], y[r_index[:size]]\n",
    "X_test, y_test = X[r_index[size:]], y[r_index[size:]]\n",
    "\n",
    "print(\"Dataset dimension:\")\n",
    "\n",
    "print(\"Training set = \", len(X_train), len(y_train))\n",
    "print(\"Test set = \", len(X_test), len(y_test))\n",
    "\n",
    "print(\"Dimension of input = \", len(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n",
      "First 5 samples:\n",
      "Sample 1: [-0.90068117  1.01900435 -1.34022653 -1.3154443 ] (Class: 0, Species: setosa)\n",
      "Sample 2: [-1.14301691 -0.13197948 -1.34022653 -1.3154443 ] (Class: 0, Species: setosa)\n",
      "Sample 3: [-1.38535265  0.32841405 -1.39706395 -1.3154443 ] (Class: 0, Species: setosa)\n",
      "Sample 4: [-1.50652052  0.09821729 -1.2833891  -1.3154443 ] (Class: 0, Species: setosa)\n",
      "Sample 5: [-1.02184904  1.24920112 -1.34022653 -1.3154443 ] (Class: 0, Species: setosa)\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature names:\", iris.feature_names)\n",
    "print(\"Target names:\", iris.target_names)\n",
    "\n",
    "print(\"First 5 samples:\")\n",
    "for i in range(5):\n",
    "    print(f\"Sample {i+1}: {X[i]} (Class: {y[i]}, Species: {iris.target_names[y[i]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters 43\n",
      "MLP of [Layer of [ReLu Neuron (4), ReLu Neuron (4), ReLu Neuron (4), ReLu Neuron (4), ReLu Neuron (4)], Layer of [Softmax Neuron (5), Softmax Neuron (5), Softmax Neuron (5)]]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(4, [5, 3])\n",
    "print('number of parameters', len(model.parameters()))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 1.2986896141830822, accuracy 24.166666666666668%\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "def loss():\n",
    "    inputs = [list(map(Value, xrow)) for xrow in X_train]\n",
    "    \n",
    "    # forward the model to get scores\n",
    "    scores = list(map(model, inputs)) \n",
    "    \n",
    "    # Cross-entropy Loss\n",
    "    losses = [(- score[yi].log()) for yi, score in zip(y_train, scores)]\n",
    "    total_loss = sum(losses) / len(losses)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = []\n",
    "    for yi, score in zip(y_train, scores):\n",
    "        argmax = -1\n",
    "        valmax = 0\n",
    "        for idx, scorei in enumerate(score):\n",
    "            if scorei.data > valmax:\n",
    "                valmax = scorei.data\n",
    "                argmax = idx\n",
    "        accuracy.append((argmax == yi))\n",
    "\n",
    "    acc = sum(accuracy) / len(accuracy)\n",
    "        \n",
    "    return total_loss, acc\n",
    "\n",
    "total_loss, acc = loss()\n",
    "print(f\"step {0} loss {total_loss.data}, accuracy {acc*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 1.2436923680712217, accuracy 25.0%\n",
      "step 10 loss 0.9642199278883227, accuracy 64.16666666666667%\n",
      "step 20 loss 0.8158705671109199, accuracy 75.83333333333333%\n",
      "step 30 loss 0.7093393116585764, accuracy 85.0%\n",
      "step 40 loss 0.619341732195803, accuracy 92.5%\n",
      "step 50 loss 0.53145548678982, accuracy 94.16666666666667%\n",
      "step 60 loss 0.44301416738477334, accuracy 95.0%\n",
      "step 70 loss 0.36822854111973863, accuracy 95.83333333333334%\n",
      "step 80 loss 0.31369323343543337, accuracy 95.83333333333334%\n",
      "step 90 loss 0.2755089199534364, accuracy 95.83333333333334%\n",
      "step 100 loss 0.24755164746623867, accuracy 96.66666666666667%\n",
      "step 110 loss 0.22596287353358785, accuracy 96.66666666666667%\n",
      "step 120 loss 0.20859950313554004, accuracy 96.66666666666667%\n",
      "step 130 loss 0.19409198193164945, accuracy 96.66666666666667%\n",
      "step 140 loss 0.18169474835561492, accuracy 96.66666666666667%\n",
      "step 150 loss 0.1708734606726693, accuracy 96.66666666666667%\n",
      "step 160 loss 0.16131724700398994, accuracy 95.83333333333334%\n",
      "step 170 loss 0.152815099258085, accuracy 95.83333333333334%\n",
      "step 180 loss 0.1452169759862901, accuracy 95.83333333333334%\n",
      "step 190 loss 0.13830766928556373, accuracy 96.66666666666667%\n",
      "step 200 loss 0.13210024373911314, accuracy 96.66666666666667%\n",
      "step 210 loss 0.12651558375066024, accuracy 96.66666666666667%\n",
      "step 220 loss 0.12153061185803217, accuracy 96.66666666666667%\n",
      "step 230 loss 0.11702758066854549, accuracy 96.66666666666667%\n",
      "step 240 loss 0.11293308460564044, accuracy 96.66666666666667%\n",
      "step 250 loss 0.10919309944206387, accuracy 96.66666666666667%\n",
      "step 260 loss 0.10577082507030872, accuracy 96.66666666666667%\n",
      "step 270 loss 0.10265267078341346, accuracy 96.66666666666667%\n",
      "step 280 loss 0.0997910402831734, accuracy 96.66666666666667%\n",
      "step 290 loss 0.0971518683786306, accuracy 96.66666666666667%\n",
      "step 300 loss 0.09473117735811362, accuracy 96.66666666666667%\n",
      "step 310 loss 0.09250118186240748, accuracy 96.66666666666667%\n",
      "step 320 loss 0.09043030289103099, accuracy 96.66666666666667%\n",
      "step 330 loss 0.0885044146974473, accuracy 96.66666666666667%\n",
      "step 340 loss 0.08671729130185285, accuracy 96.66666666666667%\n",
      "step 350 loss 0.08504815301680506, accuracy 96.66666666666667%\n",
      "step 360 loss 0.08348610166211276, accuracy 96.66666666666667%\n",
      "step 370 loss 0.08202167537742067, accuracy 96.66666666666667%\n",
      "step 380 loss 0.08064647545482388, accuracy 96.66666666666667%\n",
      "step 390 loss 0.07935301074301254, accuracy 96.66666666666667%\n"
     ]
    }
   ],
   "source": [
    "# backward\n",
    "for k in range(400):\n",
    "    model.zero_grad()\n",
    "    total_loss.backward()\n",
    "        \n",
    "    # update (sgd)\n",
    "    learning_rate = 0.1\n",
    "    for p in model.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    total_loss, acc = loss()\n",
    "    \n",
    "    if k % 10 == 0:\n",
    "        print(f\"step {k} loss {total_loss.data}, accuracy {acc*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(batch_size=None):\n",
    "    inputs = [list(map(Value, xrow)) for xrow in X_test]\n",
    "    \n",
    "    # forward the model to get scores\n",
    "    scores = list(map(model, inputs))\n",
    "    \n",
    "    # Cross-entropy Loss\n",
    "    losses = [(- score[yi].log()) for yi, score in zip(y_test, scores)]\n",
    "    total_loss = sum(losses) / len(losses)\n",
    "\n",
    "    # Accuracy\n",
    "    predict = []\n",
    "    accuracy = []\n",
    "    for yi, score in zip(y_test, scores):\n",
    "        argmax = -1\n",
    "        valmax = 0\n",
    "        for idx, scorei in enumerate(score):\n",
    "            if scorei.data > valmax:\n",
    "                valmax = scorei.data\n",
    "                argmax = idx\n",
    "        predict.append((argmax, yi))\n",
    "        accuracy.append((argmax == yi))\n",
    "\n",
    "    acc = sum(accuracy) / len(accuracy)\n",
    "        \n",
    "    return total_loss, acc, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.11123970982618364, accuracy = 96.66666666666667%\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "total_loss, acc, pred = test()\n",
    "print(f\"Loss = {total_loss.data}, accuracy = {acc*100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
